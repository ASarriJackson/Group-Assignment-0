{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection on Financial Data Sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we reflect on our collective findings from our four topics of research: \n",
    "* World Bank's Loan Programs\n",
    "* Bank Loan Prediction\n",
    "* Bank Churning Analysis \n",
    "* Customer interest in Long-Term Savings Accounts\n",
    "\n",
    "Through a comprehensive look into financial datasets, we have observed there is similarities in the types of data collected. Demographic Data was used across all datasets to profile customers, helping us to analyse whether demographics play a role in predicting customer churn, loan appprovals, and even the projected duration of debt repayment by countries. Transactional data has also played a vital role by capturing customer decisions, financial transactions and repayment plans. This is predominantly used by the World Bank, who collect repayment histories in order to build future repayment models and credit agreements. \n",
    "\n",
    "The datasets used in our final report were all sourced from public data repositries. We primarily used Kaggle due to its extensive collection of datasets, and because the ones we chose come with established code documents analysing them. This enabled us to build upon existing examples to develop a rigerous understanding of our individual topics. Additionally, the World Bank allows public access to transactional histories from various countries, allowing us to gain insights into how the World Bank makes decisions regarding credit allowences.  \n",
    "\n",
    "For our project, we selected Python as our primary tool for analysis. This was because of our collective experience and familiarity with its packages, specially designed for exploratory data analyses. In particular, we used libraries such as Pandas, NumPy, Seaborn and matplotlib, which we found were ideal for handling large datasets and creating informative visualisations. Although Pandas was relatively new to some of us, we gained insight by using the official documentation, as well as in-built help functions whenever we came across a snippet of code unfamiliar to us. We also encountered challenges while setting up our chosen development environment, Visual Studio Code, which we agreed would be the most appropriate for its seemless integration with github. Still, some of us were unfamiliar with the logistics of installing Anaconda and setting up virtual environments. But with the guidance from others, we quickly gained confidence and set off to analyse our own datasets. \n",
    "\n",
    "We used Github to share and acces each others code, which streamlined our workflow by allowing simultanious contributions to the project. Initially setting up GitHub seemed tedious, but soon afterwards it proved to be an effective way of collaberaing on our code. Fortunately we had recources available in the universities DST Github repository, which provided valuable insight on how to add our changes without corrupting the shared code. \n",
    "\n",
    "Our primary objective was to identify and use recources related to customer behaviours, building the analytical foundation which would lead us to producing forecasting and predictive models later on. Along the way, we encountered some recources that weren't relevant to our respective topics but are popular in in other areas of financial analysis. For example some datasets are collected for fraud detection purposes. The goal for banks here is to assess risks, identify suspicious activities and build a detection model capable of spotting fraudulant activities. \n",
    "\n",
    "\n",
    "After much deliberation, we decided to focus our research on the topic of bank churning. Working with this dataset has provided a great insight into a significant part of banking operations. However, we recognize that the datasets limited size may restrict the depth of analysis. And thus, we are open to building our model on a similar but larger dataset, should we find a more suitable one. In this initial analysis, we applied EDA techniques to conduct basic comparative analysis and identify the important features of our dataset. We also implemented data cleaning techniques to check for missing values and discard unhelpful variables such as \"CustomerID\" and \"Surname\", which are unique to the customer and offer no statistical value. Finally we used \n",
    "\n",
    "Moving forward, our goal is to enhance our analyis on the Bank Churn model by applying machine learning techniques, such as classification and regression. These approches will allow us to model customer behaviour and predict churn likelihood. Understanding these behavioural patterns will be beneficial for the bank to set up schemes to prevent this in future.   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
